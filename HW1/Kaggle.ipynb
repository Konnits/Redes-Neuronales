{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b31e794-7543-4513-a463-949283e77946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import random\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from torchaudio import transforms\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6572cd9-746e-4747-8d84-6dcdad6fc2da",
   "metadata": {},
   "source": [
    "# Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b8bfad-05aa-4b39-8865-ae8bc5b535bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array({'0': 'Femenino', '1': 'Masculino', '2': 'Argentina', '3': 'Chile', '4': 'Colombia', '5': 'Perú', '6': 'Venezuela'},\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/kaggle/\"\n",
    "np.load(path + \"dict.npy\", allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b67337d2-e309-4132-a5da-95aeb62d31d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + \"Train.csv\")\n",
    "df[\"Id\"] = path + \"Train/\"+df[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5922ca-677a-4425-8bdd-25433635f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label_1\"] = df[\"Expected\"].apply(lambda x : int(x.split(\" \")[0]))\n",
    "df[\"label_2\"] = df[\"Expected\"].apply(lambda x : int(x.split(\" \")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c03ee410-b943-4e6f-ae37-92d7f8b99873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Expected</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./data/kaggle/Train/01216683570.wav</td>\n",
       "      <td>0 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./data/kaggle/Train/00433588573.wav</td>\n",
       "      <td>0 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./data/kaggle/Train/00381534896.wav</td>\n",
       "      <td>0 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./data/kaggle/Train/01635825413.wav</td>\n",
       "      <td>0 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./data/kaggle/Train/00325117692.wav</td>\n",
       "      <td>0 2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Id Expected  label_1  label_2\n",
       "0  ./data/kaggle/Train/01216683570.wav      0 2        0        2\n",
       "1  ./data/kaggle/Train/00433588573.wav      0 2        0        2\n",
       "2  ./data/kaggle/Train/00381534896.wav      0 2        0        2\n",
       "3  ./data/kaggle/Train/01635825413.wav      0 2        0        2\n",
       "4  ./data/kaggle/Train/00325117692.wav      0 2        0        2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f55a60-4c78-4e48-a2ba-51afcb791c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"Id\"].apply(lambda x : torchaudio.load(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5a63bb-d406-477c-a833-5c0d9f36f6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.apply(lambda x : x[1]) == 48000).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3683b00-e579-4ba5-82ab-418ef6d56098",
   "metadata": {},
   "source": [
    "Todas los datos tienen el mismo Sample rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ba0297a-7087-4ae3-a2d1-a8687a249d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data.apply(lambda x : x[0].shape[0]) == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30568399-3b79-442b-8c28-344fdb132923",
   "metadata": {},
   "source": [
    "Todos los datos tienen la misma cantidad de canales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd0f9424-19f7-46c2-a4f0-b1cbe59e31bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audio = data.apply(lambda x : x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4001db4d-ad6c-4caa-b871-8daa5f82c75b",
   "metadata": {},
   "source": [
    "## Transformación e imputación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ceab289d-a02d-49fc-bd6a-bd127ce36670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twochannel(audio):\n",
    "    \n",
    "    if audio.shape[0] == 2:\n",
    "        return audio\n",
    "    else:\n",
    "        return torch.cat([audio, audio])\n",
    "    \n",
    "def resize_audio(audio, max_ms = 12000):\n",
    "    \n",
    "    max_len = 48000 * max_ms // 1000\n",
    "    rows, length = audio.shape\n",
    "    \n",
    "    if length > max_len:\n",
    "        audio = audio[ : , : max_len]\n",
    "    \n",
    "    elif length < max_len:\n",
    "        start = random.randint(0, max_len - length)\n",
    "        end = max_len - length - start\n",
    "        \n",
    "        pad_start = torch.zeros((rows, start))\n",
    "        pad_end = torch.zeros((rows, end))\n",
    "        \n",
    "        audio = torch.cat((pad_start, audio, pad_end), 1)\n",
    "    \n",
    "    return audio\n",
    "\n",
    "def apply_transform(data):\n",
    "    \n",
    "    data_audio_tc = data.apply(lambda x : twochannel(x))\n",
    "    return data_audio_tc.apply(lambda x : resize_audio(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38c2a74-74df-46a7-b4b1-1d42057639ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = apply_transform(data_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "90ea55fc-8fa7-448d-b772-004145b6bd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_audio(audio):\n",
    "    \n",
    "    shift = int((2 * random.random() - 1) * 0.5 * audio.shape[1])\n",
    "    return audio.roll(shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0fcbc4bd-f723-4ce0-a03c-6898da1917ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectro_audio(audio):\n",
    "    \n",
    "    spec = transforms.MelSpectrogram(48000, n_fft = 2**10, n_mels = 100)(audio)\n",
    "    return transforms.AmplitudeToDB(top_db = 80, )(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62c0e680-432c-4c9d-b4f3-f34216fad5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frec_time_mask(spec, max_mask_pct = 0.1, n_freq_masks = 1, n_time_masks = 1):\n",
    "    \n",
    "    _, n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "        aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "        aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33578bba-b67a-49ea-b35c-07d5ea2a4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(audio):\n",
    "    noise = torch.rand(audio.shape) * 2 - 1\n",
    "    return audio + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97bcca6b-727f-40cf-8d33-e04ecef679fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_data(data, labels, shifted_data = 10, funcs = [frec_time_mask, add_noise]):\n",
    "    \n",
    "    X = np.array([])\n",
    "    y = np.array([])\n",
    "    for pos, audio in enumerate(data):\n",
    "        for i in range(shifted_data):\n",
    "            aux = spectro_audio(shift_audio(audio))\n",
    "            while np.random.randint(2) == 0:\n",
    "                func = funcs[np.random.randint(2)]\n",
    "                aux = func(aux)\n",
    "            if len(X) == 0:\n",
    "                X = np.array([aux.numpy()])\n",
    "                y = np.array([[labels[pos]]])\n",
    "            else:\n",
    "                X = np.concatenate((X, np.array([aux.numpy(), aux.numpy()])), axis = 0)\n",
    "                y = np.concatenate((y, np.array([[labels[pos]]])), axis = 0)\n",
    "            del aux\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af29859e-8bf4-45aa-a97e-b1830eece7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_disordered = [(i, df[\"Expected\"][pos]) for pos, i in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17136599-5bd5-42e4-80e7-3416d5e9f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_disordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "988705b3-bba0-4bce-870c-47c548c4c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_disordered = [i[0] for i in data_disordered]\n",
    "y_disordered = [i[1] for i in data_disordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b735e94-5950-4342-8028-6f6153eab6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, data_audio, data_disordered, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5ccac9f-13fc-40f9-83e5-eb536735864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((27900, 2, 100, 1126), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafcfb04-1c90-44f1-9a66-b8e4fc504cc8",
   "metadata": {},
   "source": [
    "### Generamos la data nueva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f8474fd-0c6c-49a9-86e0-71dc095f4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1394 de 1395 exactly 27880\n",
      "1394 de 1395 exactly 27881\n",
      "1394 de 1395 exactly 27882\n",
      "1394 de 1395 exactly 27883\n",
      "1394 de 1395 exactly 27884\n",
      "1394 de 1395 exactly 27885\n",
      "1394 de 1395 exactly 27886\n",
      "1394 de 1395 exactly 27887\n",
      "1394 de 1395 exactly 27888\n",
      "1394 de 1395 exactly 27889\n",
      "1394 de 1395 exactly 27890\n",
      "1394 de 1395 exactly 27891\n",
      "1394 de 1395 exactly 27892\n",
      "1394 de 1395 exactly 27893\n",
      "1394 de 1395 exactly 27894\n",
      "1394 de 1395 exactly 27895\n",
      "1394 de 1395 exactly 27896\n",
      "1394 de 1395 exactly 27897\n",
      "1394 de 1395 exactly 27898\n",
      "1394 de 1395 exactly 27899\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for pos, audio in enumerate(X_disordered):\n",
    "    for i in range(20):\n",
    "        print(f\"{pos} de {len(X_disordered)} exactly {20 * pos + i}\")\n",
    "        aux = spectro_audio(shift_audio(audio))\n",
    "        aux = frec_time_mask(aux)\n",
    "        aux = add_noise(aux)\n",
    "        X[20 * pos + i] = aux.numpy()\n",
    "        if len(X) == 0:\n",
    "            # X = np.array([aux.numpy()])\n",
    "            y = np.array([y_disordered[pos]])\n",
    "        else:\n",
    "            # X = np.concatenate([X, np.array([aux.numpy()])], axis = 0)\n",
    "            y = np.concatenate([y, [y_disordered[pos]]], axis = 0)\n",
    "        del aux\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f782caca-3c22-4c71-acad-f7f3320954e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\files\\\\x.obj', \"wb\")\n",
    "# np.save(file, X)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56a79fae-62b5-4a6b-b13d-87f149c3bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\files\\\\y.obj', \"wb\")\n",
    "# np.save(file, y)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "573ec32b-6453-4ee5-b354-6b757bb97d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\files\\\\x.obj', \"rb\")\n",
    "X = np.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d326a3b5-0fe6-4623-a786-083d2d2cb032",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\files\\\\y.obj', \"rb\")\n",
    "y = np.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4db03-2747-4f77-867a-663c45954857",
   "metadata": {},
   "source": [
    "## Generamos modelos : Clasificar nacionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c48a5c0a-c58a-4be6-aaf2-4ed766f18d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(activation):\n",
    "    input_layer = tf.keras.layers.Input((2, 100, 1126))\n",
    "    conv_1 = tf.keras.layers.Conv2D(16, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(input_layer)\n",
    "    pooling_1 = tf.keras.layers.MaxPooling2D(padding = 'same')(conv_1)\n",
    "    conv_2 = tf.keras.layers.Conv2D(32, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(pooling_1)\n",
    "    pooling_2 = tf.keras.layers.MaxPooling2D(padding = 'same')(conv_2)\n",
    "    conv_3 = tf.keras.layers.Conv2D(64, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(pooling_2)\n",
    "    flatt = tf.keras.layers.Flatten()(conv_3)\n",
    "    hidden_1 = tf.keras.layers.Dense(128, activation = \"relu\")(flatt)\n",
    "    output_layer = tf.keras.layers.Dense(5, activation = activation)(hidden_1)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "    \n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-4),\n",
    "        metrics = tf.keras.metrics.CategoricalCrossentropy()\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9723762d-9f89-463d-bb18-48478851f0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfd6fcf1-ac19-4728-9b11-b96d4771b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 2, 100, 1126)]    0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 1, 50, 16)         162160    \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 1, 25, 16)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 13, 32)         4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 1, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 4, 64)          18496     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 218,837\n",
      "Trainable params: 218,837\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339d5ac4-616c-471c-a6cb-efbc761b115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"argentina\", \"chile\", \"colombia\", \"peru\", \"venezuela\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "857ff265-299f-4834-9636-a5f78ac1a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.DataFrame(y, columns = [\"target\"])\n",
    "y_data[\"gender\"] = y_data[\"target\"].apply(lambda x : int(x.split(\" \")[0]))\n",
    "y_data[\"nacionality\"] = y_data[\"target\"].apply(lambda x : int(x.split(\" \")[1]))\n",
    "y_data[\"argentina\"] = y_data[\"nacionality\"].apply(lambda x : int(x == 2))\n",
    "y_data[\"chile\"] = y_data[\"nacionality\"].apply(lambda x : int(x == 3))\n",
    "y_data[\"colombia\"] = y_data[\"nacionality\"].apply(lambda x : int(x == 4))\n",
    "y_data[\"peru\"] = y_data[\"nacionality\"].apply(lambda x : int(x == 5))\n",
    "y_data[\"venezuela\"] = y_data[\"nacionality\"].apply(lambda x : int(x == 6))\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y_data[cols], test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbafb361-433d-4f2b-8259-6877c6cdc74b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "785/785 [==============================] - 39s 49ms/step - loss: 1.6375 - categorical_crossentropy: 1.6375 - val_loss: 1.6124 - val_categorical_crossentropy: 1.6124\n",
      "Epoch 2/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 1.6084 - categorical_crossentropy: 1.6084 - val_loss: 1.6144 - val_categorical_crossentropy: 1.6144\n",
      "Epoch 3/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 1.6077 - categorical_crossentropy: 1.6077 - val_loss: 1.6154 - val_categorical_crossentropy: 1.6154\n",
      "Epoch 4/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 1.6037 - categorical_crossentropy: 1.6037 - val_loss: 1.6000 - val_categorical_crossentropy: 1.6000\n",
      "Epoch 5/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 1.5573 - categorical_crossentropy: 1.5573 - val_loss: 1.4405 - val_categorical_crossentropy: 1.4405\n",
      "Epoch 6/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 1.3341 - categorical_crossentropy: 1.3341 - val_loss: 1.2323 - val_categorical_crossentropy: 1.2323\n",
      "Epoch 7/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 1.1605 - categorical_crossentropy: 1.1605 - val_loss: 1.0958 - val_categorical_crossentropy: 1.0958\n",
      "Epoch 8/500\n",
      "785/785 [==============================] - 37s 48ms/step - loss: 1.0435 - categorical_crossentropy: 1.0435 - val_loss: 1.0085 - val_categorical_crossentropy: 1.0085\n",
      "Epoch 9/500\n",
      "785/785 [==============================] - 37s 48ms/step - loss: 0.9878 - categorical_crossentropy: 0.9878 - val_loss: 0.9592 - val_categorical_crossentropy: 0.9592\n",
      "Epoch 10/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.9374 - categorical_crossentropy: 0.9374 - val_loss: 0.9502 - val_categorical_crossentropy: 0.9502\n",
      "Epoch 11/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.9103 - categorical_crossentropy: 0.9103 - val_loss: 0.9543 - val_categorical_crossentropy: 0.9543\n",
      "Epoch 12/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.8736 - categorical_crossentropy: 0.8736 - val_loss: 0.8690 - val_categorical_crossentropy: 0.8690\n",
      "Epoch 13/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.8474 - categorical_crossentropy: 0.8474 - val_loss: 0.9073 - val_categorical_crossentropy: 0.9073\n",
      "Epoch 14/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.8180 - categorical_crossentropy: 0.8180 - val_loss: 0.8731 - val_categorical_crossentropy: 0.8731\n",
      "Epoch 15/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.8059 - categorical_crossentropy: 0.8059 - val_loss: 0.9145 - val_categorical_crossentropy: 0.9145\n",
      "Epoch 16/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.7749 - categorical_crossentropy: 0.7749 - val_loss: 0.9181 - val_categorical_crossentropy: 0.9181\n",
      "Epoch 17/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.7518 - categorical_crossentropy: 0.7518 - val_loss: 0.8298 - val_categorical_crossentropy: 0.8298\n",
      "Epoch 18/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.7241 - categorical_crossentropy: 0.7241 - val_loss: 0.8366 - val_categorical_crossentropy: 0.8366\n",
      "Epoch 19/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.7055 - categorical_crossentropy: 0.7055 - val_loss: 0.7986 - val_categorical_crossentropy: 0.7986\n",
      "Epoch 20/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.6777 - categorical_crossentropy: 0.6777 - val_loss: 0.8870 - val_categorical_crossentropy: 0.8870\n",
      "Epoch 21/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.6620 - categorical_crossentropy: 0.6620 - val_loss: 0.8227 - val_categorical_crossentropy: 0.8227\n",
      "Epoch 22/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.6422 - categorical_crossentropy: 0.6422 - val_loss: 0.8147 - val_categorical_crossentropy: 0.8147\n",
      "Epoch 23/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.6239 - categorical_crossentropy: 0.6239 - val_loss: 0.7876 - val_categorical_crossentropy: 0.7876\n",
      "Epoch 24/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.6031 - categorical_crossentropy: 0.6031 - val_loss: 0.7738 - val_categorical_crossentropy: 0.7738\n",
      "Epoch 25/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.5860 - categorical_crossentropy: 0.5860 - val_loss: 0.8025 - val_categorical_crossentropy: 0.8025\n",
      "Epoch 26/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.5731 - categorical_crossentropy: 0.5731 - val_loss: 0.7507 - val_categorical_crossentropy: 0.7507\n",
      "Epoch 27/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.5524 - categorical_crossentropy: 0.5524 - val_loss: 0.7671 - val_categorical_crossentropy: 0.7671\n",
      "Epoch 28/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.5442 - categorical_crossentropy: 0.5442 - val_loss: 0.7453 - val_categorical_crossentropy: 0.7453\n",
      "Epoch 29/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.5253 - categorical_crossentropy: 0.5253 - val_loss: 0.7265 - val_categorical_crossentropy: 0.7265\n",
      "Epoch 30/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.5055 - categorical_crossentropy: 0.5055 - val_loss: 0.7735 - val_categorical_crossentropy: 0.7735\n",
      "Epoch 31/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.5047 - categorical_crossentropy: 0.5047 - val_loss: 0.7731 - val_categorical_crossentropy: 0.7731\n",
      "Epoch 32/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.4855 - categorical_crossentropy: 0.4855 - val_loss: 0.7712 - val_categorical_crossentropy: 0.7712\n",
      "Epoch 33/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.4744 - categorical_crossentropy: 0.4744 - val_loss: 0.7457 - val_categorical_crossentropy: 0.7457\n",
      "Epoch 34/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.4516 - categorical_crossentropy: 0.4516 - val_loss: 0.8068 - val_categorical_crossentropy: 0.8068\n",
      "Epoch 35/500\n",
      "785/785 [==============================] - 33s 43ms/step - loss: 0.4454 - categorical_crossentropy: 0.4454 - val_loss: 0.7777 - val_categorical_crossentropy: 0.7777\n",
      "Epoch 36/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.4298 - categorical_crossentropy: 0.4298 - val_loss: 0.8126 - val_categorical_crossentropy: 0.8126\n",
      "Epoch 37/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.4175 - categorical_crossentropy: 0.4175 - val_loss: 0.7640 - val_categorical_crossentropy: 0.7640\n",
      "Epoch 38/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.4088 - categorical_crossentropy: 0.4088 - val_loss: 0.7546 - val_categorical_crossentropy: 0.7546\n",
      "Epoch 39/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.4016 - categorical_crossentropy: 0.4016 - val_loss: 0.8360 - val_categorical_crossentropy: 0.8360\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    model.fit(X, y_data[cols].values, epochs = 500, validation_split = 0.1, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6944a121-b696-426b-a60f-6173518b7856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gmendez\\Documents\\Redes neuronales\\inf395_hw1\\models\\second\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gmendez\\Documents\\Redes neuronales\\inf395_hw1\\models\\second\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\second\")\n",
    "model.save_weights(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\seconds_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b17144b-566a-41da-a202-d3e2eb5f24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = make_model(\"tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "306a63a6-de9a-4344-accf-839a0660700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = make_model(\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88f03cdc-1223-4709-b7f4-af37cfe62836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "785/785 [==============================] - 39s 49ms/step - loss: 1.5741 - categorical_crossentropy: 1.5741 - val_loss: 1.1853 - val_categorical_crossentropy: 1.1853\n",
      "Epoch 2/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 1.1656 - categorical_crossentropy: 1.1656 - val_loss: 1.1597 - val_categorical_crossentropy: 1.1597\n",
      "Epoch 3/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 1.0448 - categorical_crossentropy: 1.0448 - val_loss: 1.0016 - val_categorical_crossentropy: 1.0016\n",
      "Epoch 4/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.9620 - categorical_crossentropy: 0.9620 - val_loss: 0.9596 - val_categorical_crossentropy: 0.9596\n",
      "Epoch 5/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.9018 - categorical_crossentropy: 0.9018 - val_loss: 0.9298 - val_categorical_crossentropy: 0.9298\n",
      "Epoch 6/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.8645 - categorical_crossentropy: 0.8645 - val_loss: 0.8327 - val_categorical_crossentropy: 0.8327\n",
      "Epoch 7/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.8214 - categorical_crossentropy: 0.8214 - val_loss: 0.8784 - val_categorical_crossentropy: 0.8784\n",
      "Epoch 8/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.8013 - categorical_crossentropy: 0.8013 - val_loss: 0.8440 - val_categorical_crossentropy: 0.8440\n",
      "Epoch 9/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.7623 - categorical_crossentropy: 0.7623 - val_loss: 0.8036 - val_categorical_crossentropy: 0.8036\n",
      "Epoch 10/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.7222 - categorical_crossentropy: 0.7222 - val_loss: 0.7639 - val_categorical_crossentropy: 0.7639\n",
      "Epoch 11/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.6873 - categorical_crossentropy: 0.6873 - val_loss: 0.7206 - val_categorical_crossentropy: 0.7206\n",
      "Epoch 12/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.6541 - categorical_crossentropy: 0.6541 - val_loss: 0.7100 - val_categorical_crossentropy: 0.7100\n",
      "Epoch 13/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.6146 - categorical_crossentropy: 0.6146 - val_loss: 0.7157 - val_categorical_crossentropy: 0.7157\n",
      "Epoch 14/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.5917 - categorical_crossentropy: 0.5917 - val_loss: 0.8172 - val_categorical_crossentropy: 0.8172\n",
      "Epoch 15/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.5821 - categorical_crossentropy: 0.5821 - val_loss: 0.6912 - val_categorical_crossentropy: 0.6912\n",
      "Epoch 16/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.5492 - categorical_crossentropy: 0.5492 - val_loss: 0.6628 - val_categorical_crossentropy: 0.6628\n",
      "Epoch 17/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.5217 - categorical_crossentropy: 0.5217 - val_loss: 0.8119 - val_categorical_crossentropy: 0.8119\n",
      "Epoch 18/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.4982 - categorical_crossentropy: 0.4982 - val_loss: 0.7121 - val_categorical_crossentropy: 0.7121\n",
      "Epoch 19/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.4813 - categorical_crossentropy: 0.4813 - val_loss: 0.6318 - val_categorical_crossentropy: 0.6318\n",
      "Epoch 20/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.4590 - categorical_crossentropy: 0.4590 - val_loss: 0.6075 - val_categorical_crossentropy: 0.6075\n",
      "Epoch 21/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.4494 - categorical_crossentropy: 0.4494 - val_loss: 0.5983 - val_categorical_crossentropy: 0.5983\n",
      "Epoch 22/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.4262 - categorical_crossentropy: 0.4262 - val_loss: 0.6030 - val_categorical_crossentropy: 0.6030\n",
      "Epoch 23/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.4102 - categorical_crossentropy: 0.4102 - val_loss: 0.6684 - val_categorical_crossentropy: 0.6684\n",
      "Epoch 24/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.3964 - categorical_crossentropy: 0.3964 - val_loss: 0.6050 - val_categorical_crossentropy: 0.6050\n",
      "Epoch 25/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.3771 - categorical_crossentropy: 0.3771 - val_loss: 0.6373 - val_categorical_crossentropy: 0.6373\n",
      "Epoch 26/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.3657 - categorical_crossentropy: 0.3657 - val_loss: 0.6044 - val_categorical_crossentropy: 0.6044\n",
      "Epoch 27/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.3621 - categorical_crossentropy: 0.3621 - val_loss: 0.5714 - val_categorical_crossentropy: 0.5714\n",
      "Epoch 28/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.3339 - categorical_crossentropy: 0.3339 - val_loss: 0.6701 - val_categorical_crossentropy: 0.6701\n",
      "Epoch 29/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.3399 - categorical_crossentropy: 0.3399 - val_loss: 0.7416 - val_categorical_crossentropy: 0.7416\n",
      "Epoch 30/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.3180 - categorical_crossentropy: 0.3180 - val_loss: 0.6470 - val_categorical_crossentropy: 0.6470\n",
      "Epoch 31/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.3055 - categorical_crossentropy: 0.3055 - val_loss: 0.5615 - val_categorical_crossentropy: 0.5615\n",
      "Epoch 32/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.2958 - categorical_crossentropy: 0.2958 - val_loss: 0.6443 - val_categorical_crossentropy: 0.6443\n",
      "Epoch 33/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.2881 - categorical_crossentropy: 0.2881 - val_loss: 0.5728 - val_categorical_crossentropy: 0.5728\n",
      "Epoch 34/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.2793 - categorical_crossentropy: 0.2793 - val_loss: 0.6361 - val_categorical_crossentropy: 0.6361\n",
      "Epoch 35/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.2711 - categorical_crossentropy: 0.2711 - val_loss: 0.4968 - val_categorical_crossentropy: 0.4968\n",
      "Epoch 36/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.2596 - categorical_crossentropy: 0.2596 - val_loss: 0.6454 - val_categorical_crossentropy: 0.6454\n",
      "Epoch 37/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.2526 - categorical_crossentropy: 0.2526 - val_loss: 0.6584 - val_categorical_crossentropy: 0.6584\n",
      "Epoch 38/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.2481 - categorical_crossentropy: 0.2481 - val_loss: 0.6470 - val_categorical_crossentropy: 0.6470\n",
      "Epoch 39/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.2278 - categorical_crossentropy: 0.2278 - val_loss: 0.5940 - val_categorical_crossentropy: 0.5940\n",
      "Epoch 40/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.2288 - categorical_crossentropy: 0.2288 - val_loss: 0.6209 - val_categorical_crossentropy: 0.6209\n",
      "Epoch 41/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.2273 - categorical_crossentropy: 0.2273 - val_loss: 0.5770 - val_categorical_crossentropy: 0.5770\n",
      "Epoch 42/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.2082 - categorical_crossentropy: 0.2082 - val_loss: 0.7611 - val_categorical_crossentropy: 0.7611\n",
      "Epoch 43/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.2097 - categorical_crossentropy: 0.2097 - val_loss: 0.5496 - val_categorical_crossentropy: 0.5496\n",
      "Epoch 44/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.2146 - categorical_crossentropy: 0.2146 - val_loss: 0.5437 - val_categorical_crossentropy: 0.5437\n",
      "Epoch 45/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.1864 - categorical_crossentropy: 0.1864 - val_loss: 0.5067 - val_categorical_crossentropy: 0.5067\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    second_model.fit(X, y_data[cols].values, epochs = 500, validation_split = 0.1, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7456c182-769f-46e2-91a4-2f6db1033d1d",
   "metadata": {},
   "source": [
    "Este modelo parece ser el que mejor resultados obtiene en el Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03f34cc7-de70-4c95-bfdf-44e8f462ba05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gmendez\\Documents\\Redes neuronales\\inf395_hw1\\models\\third\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gmendez\\Documents\\Redes neuronales\\inf395_hw1\\models\\third\\assets\n"
     ]
    }
   ],
   "source": [
    "second_model.save(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\third\")\n",
    "second_model.save_weights(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\third_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ed04558-5689-480b-9614-883f7910cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(activation):\n",
    "    input_layer = tf.keras.layers.Input((2, 100, 1126))\n",
    "    conv_1 = tf.keras.layers.Conv2D(16, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(input_layer)\n",
    "    pooling_1 = tf.keras.layers.MaxPooling2D(padding = 'same')(conv_1)\n",
    "    conv_2 = tf.keras.layers.Conv2D(32, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(pooling_1)\n",
    "    pooling_2 = tf.keras.layers.MaxPooling2D(padding = 'same')(conv_2)\n",
    "    conv_3 = tf.keras.layers.Conv2D(64, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(pooling_2)\n",
    "    flatt = tf.keras.layers.Flatten()(conv_3)\n",
    "    hidden_1 = tf.keras.layers.Dense(256, activation = \"relu\")(flatt)\n",
    "    hidden_2 = tf.keras.layers.Dense(128, activation = \"relu\")(hidden_1)\n",
    "    output_layer = tf.keras.layers.Dense(5, activation = activation)(hidden_2)\n",
    "    \n",
    "    model = tf.keras.Model(inputs = input_layer, outputs = output_layer)\n",
    "    \n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-4),\n",
    "        metrics = tf.keras.metrics.CategoricalCrossentropy()\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2893633-370e-447f-b3c7-fce30ffb4c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "third_model = make_model(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5dbe6799-6259-4eb7-9090-736fdcd1f511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 1.6143 - categorical_crossentropy: 1.6143 - val_loss: 1.6200 - val_categorical_crossentropy: 1.6200\n",
      "Epoch 2/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 1.6080 - categorical_crossentropy: 1.6080 - val_loss: 1.6170 - val_categorical_crossentropy: 1.6170\n",
      "Epoch 3/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 1.6082 - categorical_crossentropy: 1.6082 - val_loss: 1.6179 - val_categorical_crossentropy: 1.6179\n",
      "Epoch 4/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 1.6072 - categorical_crossentropy: 1.6072 - val_loss: 1.6142 - val_categorical_crossentropy: 1.6142\n",
      "Epoch 5/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 1.6033 - categorical_crossentropy: 1.6033 - val_loss: 1.6150 - val_categorical_crossentropy: 1.6150\n",
      "Epoch 6/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 1.5846 - categorical_crossentropy: 1.5846 - val_loss: 1.5873 - val_categorical_crossentropy: 1.5873\n",
      "Epoch 7/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 1.4835 - categorical_crossentropy: 1.4835 - val_loss: 1.3544 - val_categorical_crossentropy: 1.3544\n",
      "Epoch 8/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 1.3360 - categorical_crossentropy: 1.3360 - val_loss: 1.2608 - val_categorical_crossentropy: 1.2608\n",
      "Epoch 9/500\n",
      "785/785 [==============================] - 38s 49ms/step - loss: 1.2075 - categorical_crossentropy: 1.2075 - val_loss: 1.2022 - val_categorical_crossentropy: 1.2022\n",
      "Epoch 10/500\n",
      "785/785 [==============================] - 38s 49ms/step - loss: 1.1139 - categorical_crossentropy: 1.1139 - val_loss: 1.1000 - val_categorical_crossentropy: 1.1000\n",
      "Epoch 11/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 1.0238 - categorical_crossentropy: 1.0238 - val_loss: 1.0010 - val_categorical_crossentropy: 1.0010\n",
      "Epoch 12/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.9310 - categorical_crossentropy: 0.9310 - val_loss: 0.9241 - val_categorical_crossentropy: 0.9241\n",
      "Epoch 13/500\n",
      "785/785 [==============================] - 39s 49ms/step - loss: 0.8607 - categorical_crossentropy: 0.8607 - val_loss: 0.8659 - val_categorical_crossentropy: 0.8659\n",
      "Epoch 14/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.8237 - categorical_crossentropy: 0.8237 - val_loss: 0.8964 - val_categorical_crossentropy: 0.8964\n",
      "Epoch 15/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.7812 - categorical_crossentropy: 0.7812 - val_loss: 0.8512 - val_categorical_crossentropy: 0.8512\n",
      "Epoch 16/500\n",
      "785/785 [==============================] - 38s 49ms/step - loss: 0.7541 - categorical_crossentropy: 0.7541 - val_loss: 0.9132 - val_categorical_crossentropy: 0.9132\n",
      "Epoch 17/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.7235 - categorical_crossentropy: 0.7235 - val_loss: 0.8592 - val_categorical_crossentropy: 0.8592\n",
      "Epoch 18/500\n",
      "785/785 [==============================] - 39s 49ms/step - loss: 0.6942 - categorical_crossentropy: 0.6942 - val_loss: 0.7916 - val_categorical_crossentropy: 0.7916\n",
      "Epoch 19/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.6781 - categorical_crossentropy: 0.6781 - val_loss: 0.8341 - val_categorical_crossentropy: 0.8341\n",
      "Epoch 20/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.6555 - categorical_crossentropy: 0.6555 - val_loss: 0.7954 - val_categorical_crossentropy: 0.7954\n",
      "Epoch 21/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.6322 - categorical_crossentropy: 0.6322 - val_loss: 0.8292 - val_categorical_crossentropy: 0.8292\n",
      "Epoch 22/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.6184 - categorical_crossentropy: 0.6184 - val_loss: 0.8057 - val_categorical_crossentropy: 0.8057\n",
      "Epoch 23/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.6067 - categorical_crossentropy: 0.6067 - val_loss: 0.8018 - val_categorical_crossentropy: 0.8018\n",
      "Epoch 24/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.5776 - categorical_crossentropy: 0.5776 - val_loss: 0.9640 - val_categorical_crossentropy: 0.9640\n",
      "Epoch 25/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.5692 - categorical_crossentropy: 0.5692 - val_loss: 0.8301 - val_categorical_crossentropy: 0.8301\n",
      "Epoch 26/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.5545 - categorical_crossentropy: 0.5545 - val_loss: 0.8120 - val_categorical_crossentropy: 0.8120\n",
      "Epoch 27/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.5445 - categorical_crossentropy: 0.5445 - val_loss: 0.9145 - val_categorical_crossentropy: 0.9145\n",
      "Epoch 28/500\n",
      "785/785 [==============================] - 39s 49ms/step - loss: 0.5230 - categorical_crossentropy: 0.5230 - val_loss: 0.8515 - val_categorical_crossentropy: 0.8515\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    third_model.fit(X, y_data[cols].values, epochs = 500, validation_split = 0.1, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18ac511b-4e0a-4178-b6b3-d17557e1e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_model = make_model(\"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c03b1670-bbeb-4a05-a8df-04947a222ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "785/785 [==============================] - 37s 44ms/step - loss: 1.6114 - categorical_crossentropy: 1.6114 - val_loss: 1.6160 - val_categorical_crossentropy: 1.6160\n",
      "Epoch 2/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 1.3548 - categorical_crossentropy: 1.3548 - val_loss: 1.1044 - val_categorical_crossentropy: 1.1044\n",
      "Epoch 3/500\n",
      "785/785 [==============================] - 38s 49ms/step - loss: 1.0932 - categorical_crossentropy: 1.0932 - val_loss: 1.1389 - val_categorical_crossentropy: 1.1389\n",
      "Epoch 4/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 1.0192 - categorical_crossentropy: 1.0192 - val_loss: 1.0174 - val_categorical_crossentropy: 1.0174\n",
      "Epoch 5/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.9668 - categorical_crossentropy: 0.9668 - val_loss: 0.9546 - val_categorical_crossentropy: 0.9546\n",
      "Epoch 6/500\n",
      "785/785 [==============================] - 38s 48ms/step - loss: 0.9062 - categorical_crossentropy: 0.9062 - val_loss: 0.9639 - val_categorical_crossentropy: 0.9639\n",
      "Epoch 7/500\n",
      "785/785 [==============================] - 39s 49ms/step - loss: 0.8585 - categorical_crossentropy: 0.8585 - val_loss: 0.8508 - val_categorical_crossentropy: 0.8508\n",
      "Epoch 8/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.8161 - categorical_crossentropy: 0.8161 - val_loss: 0.8411 - val_categorical_crossentropy: 0.8411\n",
      "Epoch 9/500\n",
      "785/785 [==============================] - 35s 44ms/step - loss: 0.7622 - categorical_crossentropy: 0.7622 - val_loss: 0.8091 - val_categorical_crossentropy: 0.8091\n",
      "Epoch 10/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.7231 - categorical_crossentropy: 0.7231 - val_loss: 0.8195 - val_categorical_crossentropy: 0.8195\n",
      "Epoch 11/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.6802 - categorical_crossentropy: 0.6802 - val_loss: 0.7648 - val_categorical_crossentropy: 0.7648\n",
      "Epoch 12/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.6595 - categorical_crossentropy: 0.6595 - val_loss: 0.8127 - val_categorical_crossentropy: 0.8127\n",
      "Epoch 13/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.6186 - categorical_crossentropy: 0.6186 - val_loss: 0.7647 - val_categorical_crossentropy: 0.7647\n",
      "Epoch 14/500\n",
      "785/785 [==============================] - 38s 49ms/step - loss: 0.5892 - categorical_crossentropy: 0.5892 - val_loss: 0.7687 - val_categorical_crossentropy: 0.7687\n",
      "Epoch 15/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.5568 - categorical_crossentropy: 0.5568 - val_loss: 0.7695 - val_categorical_crossentropy: 0.7695\n",
      "Epoch 16/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.5256 - categorical_crossentropy: 0.5256 - val_loss: 0.7154 - val_categorical_crossentropy: 0.7154\n",
      "Epoch 17/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.5057 - categorical_crossentropy: 0.5057 - val_loss: 0.8112 - val_categorical_crossentropy: 0.8112\n",
      "Epoch 18/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.4769 - categorical_crossentropy: 0.4769 - val_loss: 0.7330 - val_categorical_crossentropy: 0.7330\n",
      "Epoch 19/500\n",
      "785/785 [==============================] - 37s 47ms/step - loss: 0.4517 - categorical_crossentropy: 0.4517 - val_loss: 0.7337 - val_categorical_crossentropy: 0.7337\n",
      "Epoch 20/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.4249 - categorical_crossentropy: 0.4249 - val_loss: 0.7484 - val_categorical_crossentropy: 0.7484\n",
      "Epoch 21/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.4000 - categorical_crossentropy: 0.4000 - val_loss: 0.7636 - val_categorical_crossentropy: 0.7636\n",
      "Epoch 22/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.3878 - categorical_crossentropy: 0.3878 - val_loss: 0.7413 - val_categorical_crossentropy: 0.7413\n",
      "Epoch 23/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.3640 - categorical_crossentropy: 0.3640 - val_loss: 0.8099 - val_categorical_crossentropy: 0.8099\n",
      "Epoch 24/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.3492 - categorical_crossentropy: 0.3492 - val_loss: 0.7796 - val_categorical_crossentropy: 0.7796\n",
      "Epoch 25/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.3190 - categorical_crossentropy: 0.3190 - val_loss: 0.7724 - val_categorical_crossentropy: 0.7724\n",
      "Epoch 26/500\n",
      "785/785 [==============================] - 36s 46ms/step - loss: 0.3085 - categorical_crossentropy: 0.3085 - val_loss: 0.7862 - val_categorical_crossentropy: 0.7862\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    fourth_model.fit(X, y_data[cols].values, epochs = 500, validation_split = 0.1, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856d9942-e7d5-49a5-a497-3e49727c0897",
   "metadata": {},
   "source": [
    "## Generamos modelos : Clasificar género"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d6960c26-b7b5-4319-b47d-4d284ff7d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gender_model(activation):\n",
    "    \n",
    "    input_layer = tf.keras.layers.Input((2, 100, 1126))\n",
    "    conv_1 = tf.keras.layers.Conv2D(16, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(input_layer)\n",
    "    pooling_1 = tf.keras.layers.MaxPooling2D(padding = 'same')(conv_1)\n",
    "    conv_2 = tf.keras.layers.Conv2D(32, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(pooling_1)\n",
    "    pooling_2 = tf.keras.layers.MaxPooling2D(padding = 'same')(conv_2)\n",
    "    conv_3 = tf.keras.layers.Conv2D(64, 3, strides = (2,2), padding = \"same\", activation = \"relu\")(pooling_2)\n",
    "    flatt = tf.keras.layers.Flatten()(conv_3)\n",
    "    hidden_1 = tf.keras.layers.Dense(256, activation = \"relu\")(flatt)\n",
    "    hidden_2 = tf.keras.layers.Dense(128, activation = \"relu\")(hidden_1)\n",
    "    output_layer = tf.keras.layers.Dense(1, activation = activation)(hidden_2)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    \n",
    "    model.compile(\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = 2e-4),\n",
    "        metrics = tf.keras.metrics.BinaryAccuracy()\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c6e7c66-d16c-4115-9484-e5392e8cea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model_new = make_gender_model(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b42fcd1d-be16-4ace-9549-eb861dd67df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_9 (InputLayer)        [(None, 2, 100, 1126)]    0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 1, 50, 16)         162160    \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 1, 25, 16)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 1, 13, 32)         4640      \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 1, 7, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 1, 4, 64)          18496     \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 284,113\n",
      "Trainable params: 284,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gender_model_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c9a6ae7-5f76-4888-8b8d-68177e358228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "785/785 [==============================] - 34s 42ms/step - loss: 0.3461 - binary_accuracy: 0.8484 - val_loss: 0.2290 - val_binary_accuracy: 0.9186\n",
      "Epoch 2/500\n",
      "785/785 [==============================] - 40s 52ms/step - loss: 0.2218 - binary_accuracy: 0.9143 - val_loss: 0.4047 - val_binary_accuracy: 0.8183\n",
      "Epoch 3/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.1933 - binary_accuracy: 0.9274 - val_loss: 0.2186 - val_binary_accuracy: 0.9115\n",
      "Epoch 4/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.1726 - binary_accuracy: 0.9356 - val_loss: 0.1622 - val_binary_accuracy: 0.9376\n",
      "Epoch 5/500\n",
      "785/785 [==============================] - 36s 45ms/step - loss: 0.1595 - binary_accuracy: 0.9399 - val_loss: 0.1732 - val_binary_accuracy: 0.9401\n",
      "Epoch 6/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.1508 - binary_accuracy: 0.9452 - val_loss: 0.1506 - val_binary_accuracy: 0.9416\n",
      "Epoch 7/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.1283 - binary_accuracy: 0.9511 - val_loss: 0.1663 - val_binary_accuracy: 0.9330\n",
      "Epoch 8/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.1156 - binary_accuracy: 0.9571 - val_loss: 0.2389 - val_binary_accuracy: 0.9118\n",
      "Epoch 9/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.1126 - binary_accuracy: 0.9583 - val_loss: 0.1863 - val_binary_accuracy: 0.9272\n",
      "Epoch 10/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.1010 - binary_accuracy: 0.9613 - val_loss: 0.1177 - val_binary_accuracy: 0.9563\n",
      "Epoch 11/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.0940 - binary_accuracy: 0.9643 - val_loss: 0.1202 - val_binary_accuracy: 0.9530\n",
      "Epoch 12/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.0932 - binary_accuracy: 0.9646 - val_loss: 0.2759 - val_binary_accuracy: 0.9111\n",
      "Epoch 13/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.0820 - binary_accuracy: 0.9688 - val_loss: 0.1257 - val_binary_accuracy: 0.9527\n",
      "Epoch 14/500\n",
      "785/785 [==============================] - 35s 45ms/step - loss: 0.0704 - binary_accuracy: 0.9732 - val_loss: 0.1726 - val_binary_accuracy: 0.9366\n",
      "Epoch 15/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.0755 - binary_accuracy: 0.9704 - val_loss: 0.1444 - val_binary_accuracy: 0.9487\n",
      "Epoch 16/500\n",
      "785/785 [==============================] - 34s 43ms/step - loss: 0.0609 - binary_accuracy: 0.9767 - val_loss: 0.0991 - val_binary_accuracy: 0.9631\n",
      "Epoch 17/500\n",
      "785/785 [==============================] - 39s 49ms/step - loss: 0.0633 - binary_accuracy: 0.9760 - val_loss: 0.1197 - val_binary_accuracy: 0.9588\n",
      "Epoch 18/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.0591 - binary_accuracy: 0.9778 - val_loss: 0.1242 - val_binary_accuracy: 0.9573\n",
      "Epoch 19/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.0556 - binary_accuracy: 0.9789 - val_loss: 0.1487 - val_binary_accuracy: 0.9505\n",
      "Epoch 20/500\n",
      "785/785 [==============================] - 34s 44ms/step - loss: 0.0508 - binary_accuracy: 0.9812 - val_loss: 0.2419 - val_binary_accuracy: 0.9165\n",
      "Epoch 21/500\n",
      "785/785 [==============================] - 33s 43ms/step - loss: 0.0553 - binary_accuracy: 0.9791 - val_loss: 0.1071 - val_binary_accuracy: 0.9570\n",
      "Epoch 22/500\n",
      "785/785 [==============================] - 33s 43ms/step - loss: 0.0453 - binary_accuracy: 0.9832 - val_loss: 0.1457 - val_binary_accuracy: 0.9441\n",
      "Epoch 23/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.0414 - binary_accuracy: 0.9844 - val_loss: 0.1640 - val_binary_accuracy: 0.9434\n",
      "Epoch 24/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.0451 - binary_accuracy: 0.9824 - val_loss: 0.1172 - val_binary_accuracy: 0.9602\n",
      "Epoch 25/500\n",
      "785/785 [==============================] - 33s 42ms/step - loss: 0.0368 - binary_accuracy: 0.9863 - val_loss: 0.1296 - val_binary_accuracy: 0.9588\n",
      "Epoch 26/500\n",
      "785/785 [==============================] - 33s 43ms/step - loss: 0.0377 - binary_accuracy: 0.9853 - val_loss: 0.1341 - val_binary_accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/CPU:0\"):\n",
    "    gender_model_new.fit(X, y_data[\"gender\"].values, epochs = 500, validation_split = 0.1, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e215055-a82a-49ab-9e21-36a6e9e8e1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gmendez\\Documents\\Redes neuronales\\inf395_hw1\\models\\Gender_model_v2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\gmendez\\Documents\\Redes neuronales\\inf395_hw1\\models\\Gender_model_v2\\assets\n"
     ]
    }
   ],
   "source": [
    "gender_model_new.save(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\Gender_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2ef36cc9-f1e4-4e1d-b02a-944e71a3eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_model_new.save_weights(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\gender_v2_weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd17f20-87d2-45b6-b7a6-ef713ba58e8e",
   "metadata": {},
   "source": [
    "## Juntamos los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd818f30-8aa7-4edb-aad5-fa63b1e5003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nac_model = tf.keras.models.load_model(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\third\")\n",
    "gender_model = tf.keras.models.load_model(\"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\models\\\\Gender_model_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "443fee7e-631f-426c-b598-83a1874708ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input((2, 100, 1126))\n",
    "output_layer = tf.concat([gender_model(input_layer), nac_model(input_layer)], 1)\n",
    "\n",
    "model = tf.keras.models.Model(inputs = input_layer, outputs = output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8b1df8ef-aae6-4540-aad4-9c8add6f3a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12979593-1f17-43c5-9670-4e4a28ba0e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\gmendez\\\\Documents\\\\Redes neuronales\\\\inf395_hw1\\\\data\\\\kaggle\\\\\"\n",
    "df = pd.read_csv(path + \"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a857623a-3510-428b-a9e5-35ca98fbe2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df[\"Id\"].apply(lambda x : torchaudio.load(path+ \"Test\\\\\" + x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49a90935-352d-4d87-852f-cbf3b1eda2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = apply_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79fc9d0d-36fd-4284-b552-156074995306",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test.apply(lambda x : spectro_audio(x).numpy()).values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2292565d-ee45-48a1-88a3-500fa2ebc911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 5s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ffd37420-0b76-40f3-b247-1463ee4ad39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for gender, n1, n2, n3, n4, n5 in predictions:\n",
    "    m = np.argmax([n1, n2, n3, n4, n5])\n",
    "    p.append(f\"{int(gender)} {m + 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d336cd12-6421-434a-9cba-9787e6a5a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Id\" : df[\"Id\"].values, \"Expected\" : p}).to_csv(\"predictions_v2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
